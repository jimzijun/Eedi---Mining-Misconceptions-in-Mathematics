{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimzijun/Eedi---Mining-Misconceptions-in-Mathematics/blob/main/exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GgbGM94Zh3b3",
        "outputId": "1235eaef-bf3f-4cfe-e1f4-f1b019d242a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m681.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle scikit-learn datasets transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ywD6A7E-h3b4"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions download -c eedi-mining-misconceptions-in-mathematics -p ./datasets/eedi\n",
        "# !unzip ./datasets/eedi/eedi-mining-misconceptions-in-mathematics.zip -d ./datasets/eedi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fitim19BiduB",
        "outputId": "cccc2f61-640d-4139-f397-c65f541ee716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_root_path = '/content/drive/MyDrive/datasets/eedi'"
      ],
      "metadata": {
        "id": "1YWlt0WiihOC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1JlPEyXOh3b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if a GPU is available and move model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e_sx2ABbh3b4",
        "outputId": "cf3433ba-6d66-4f76-fadf-34b6bc47a16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'datasets/eedi/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-169dd4012638>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/eedi/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/eedi/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmisconception_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/eedi/misconception_mapping.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/eedi/train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('datasets/eedi/train.csv')\n",
        "test_df = pd.read_csv('datasets/eedi/test.csv')\n",
        "misconception_df = pd.read_csv('datasets/eedi/misconception_mapping.csv')\n",
        "sample_submission_df = pd.read_csv('datasets/eedi/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "segVshpSh3b4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df = train_df.sample(n=200, random_state=42)\n",
        "# Splitting 10% for evaluation, 90% for training\n",
        "train_df, eval_df = train_test_split(train_df, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPCY-ukXh3b4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def preprocess_dataframe(df, misconception_df=None, train_flag=False):\n",
        "    # List of options\n",
        "    options = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    # List of columns to keep\n",
        "    id_vars = ['QuestionId', 'QuestionText', 'ConstructId', 'ConstructName', 'SubjectId', 'SubjectName', 'CorrectAnswer']\n",
        "\n",
        "    # Initialize an empty list to collect data\n",
        "    data_list = []\n",
        "\n",
        "    # Loop over each option to collect data\n",
        "    for option in options:\n",
        "        answer_col = f'Answer{option}Text'\n",
        "        misconception_col = f'Misconception{option}Id'\n",
        "\n",
        "        # Check if the misconception column exists\n",
        "        if misconception_col in df.columns:\n",
        "            temp_df = df[id_vars + [misconception_col, answer_col]].copy()\n",
        "            temp_df.rename(columns={\n",
        "                misconception_col: 'MisconceptionId',\n",
        "                answer_col: 'AnswerText'\n",
        "            }, inplace=True)\n",
        "        else:\n",
        "            # Only include the answer column if misconception column doesn't exist\n",
        "            temp_df = df[id_vars + [answer_col]].copy()\n",
        "            temp_df['MisconceptionId'] = None  # Assign None to MisconceptionId\n",
        "            temp_df.rename(columns={\n",
        "                answer_col: 'AnswerText'\n",
        "            }, inplace=True)\n",
        "\n",
        "        temp_df['Option'] = option\n",
        "        data_list.append(temp_df)\n",
        "\n",
        "    # Concatenate all the data into a single DataFrame\n",
        "    df_combined = pd.concat(data_list, ignore_index=True)\n",
        "\n",
        "    # Exclude the rows where the option matches the correct answer\n",
        "    df_combined = df_combined[df_combined['Option'] != df_combined['CorrectAnswer']]\n",
        "\n",
        "    # If train_flag is True, merge with 'misconception_df' on 'MisconceptionId'\n",
        "    if train_flag and misconception_df is not None and 'MisconceptionId' in df_combined.columns:\n",
        "        df_combined = df_combined.merge(misconception_df, on='MisconceptionId', how='left')\n",
        "\n",
        "        # Drop rows with missing 'MisconceptionName' (only for training data)\n",
        "        if 'MisconceptionName' in df_combined.columns:\n",
        "            df_combined = df_combined.dropna(subset=['MisconceptionName'])\n",
        "    else:\n",
        "        # For testing data, add a placeholder for 'MisconceptionName'\n",
        "        df_combined['MisconceptionName'] = None\n",
        "\n",
        "    # Sort and reset index if desired\n",
        "    df_combined = df_combined.sort_values([\"QuestionId\", \"Option\"]).reset_index(drop=True)\n",
        "\n",
        "    return df_combined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2jQrP_th3b5"
      },
      "outputs": [],
      "source": [
        "train_procressed_df = preprocess_dataframe(train_df, misconception_df, train_flag=True)\n",
        "eval_procressed_df = preprocess_dataframe(eval_df, misconception_df, train_flag=True)\n",
        "test_procressed_df = preprocess_dataframe(test_df, misconception_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5TLzZEdh3b5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_row_for_qa_token(row, train_flag=False):\n",
        "    # Replace newlines in all relevant fields\n",
        "    construct_name = row['ConstructName'].replace('\\n', ' ')\n",
        "    subject_name = row['SubjectName'].replace('\\n', ' ')\n",
        "    question_text = row['QuestionText'].replace('\\n', ' ')\n",
        "    answer_text = row['AnswerText'].replace('\\n', ' ')\n",
        "\n",
        "    # Create a prompt for the question_text\n",
        "    question_prompt = (f\"Given the following context:\\n\"\n",
        "                       f\"Construct: {construct_name}, Subject: {subject_name}.\\n\"\n",
        "                       f\"Question: {question_text}\\n\"\n",
        "                       f\"Answer: {answer_text}\\n\"\n",
        "                       f\"Please predict the misconception.\")\n",
        "\n",
        "    # Set answer_text as the misconception for training\n",
        "    if train_flag and row['MisconceptionName'] is not None:\n",
        "        misconception_name = row['MisconceptionName'].replace('\\n', ' ')\n",
        "    else:\n",
        "        misconception_name = ''\n",
        "\n",
        "    # Return a DataFrame with question_text (prompt) and answer_text (misconception)\n",
        "    return pd.DataFrame({\n",
        "        'question_text': [question_prompt],\n",
        "        'answer_text': [misconception_name] if train_flag else [None]  # Use None during inference\n",
        "    })\n",
        "\n",
        "# Process the entire DataFrame for QA preparation\n",
        "def process_dataframe_for_qa_token(df, train_flag=False):\n",
        "    processed_rows = []\n",
        "\n",
        "    # Loop through each row in the input DataFrame\n",
        "    for _, row in df.iterrows():\n",
        "        processed_row = process_row_for_qa_token(row, train_flag)\n",
        "        processed_rows.append(processed_row)\n",
        "\n",
        "    # Concatenate the results into a final DataFrame\n",
        "    final_df = pd.concat(processed_rows, ignore_index=True)\n",
        "\n",
        "    return final_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R60X9CrXh3b5"
      },
      "outputs": [],
      "source": [
        "TRAIN_DF = 'data/train.csv'\n",
        "EVAL_DF = 'data/eval.csv'\n",
        "TEST_DF = 'data/test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWNlIH0Vh3b5"
      },
      "outputs": [],
      "source": [
        "process_dataframe_for_qa_token(train_procressed_df, train_flag=True).to_csv(TRAIN_DF, index=False)\n",
        "process_dataframe_for_qa_token(eval_procressed_df, train_flag=True).to_csv(EVAL_DF, index=False)\n",
        "process_dataframe_for_qa_token(test_procressed_df, train_flag=False).to_csv(TEST_DF, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcDiPRCph3b5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load your data\n",
        "train_df = pd.read_csv(TRAIN_DF)  # Contains 'question_text' and 'answer_text'\n",
        "eval_df = pd.read_csv(EVAL_DF)    # Optional evaluation data\n",
        "test_df = pd.read_csv(TEST_DF).fillna('')    # Contains 'question_text' only\n",
        "\n",
        "# Convert pandas DataFrames to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "if not eval_df.empty:\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaC4sbDVh3b5",
        "outputId": "5fdb99bf-7727-484b-a363-baf3e8634a4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/spiece.model\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer_config.json\n",
            "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Map:   0%|          | 0/419 [00:00<?, ? examples/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 419/419 [00:00<00:00, 2349.11 examples/s]\n",
            "Map: 100%|██████████| 47/47 [00:00<00:00, 2669.00 examples/s]\n",
            "Map: 100%|██████████| 9/9 [00:00<00:00, 1261.70 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "# Tokenization function\n",
        "def preprocess_function(examples):\n",
        "    inputs = [q for q in examples[\"question_text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # For training data, add labels\n",
        "    if \"answer_text\" in examples:\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(examples[\"answer_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxC24Rqch3b5",
        "outputId": "84ee6824-8bca-4357-8fd9-b7e4a432975f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/config.json\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"google/flan-t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.45.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/model.safetensors\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuX6ZyuHh3b5",
        "outputId": "e31aa687-ad47-4899-8518-39e141e1a1af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\" if not eval_df.empty else \"no\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    push_to_hub=False,\n",
        "    logging_dir='./logs',  # Directory for storing logs\n",
        "    logging_steps=50,  # Log every 50 steps\n",
        "    log_level='info',  # Logging level\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYp2dkyh3b5"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9U6ufW1h3b5",
        "outputId": "2ddf0738-763d-49c7-f474-c362c65e3c9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 419\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 135\n",
            "  Number of trainable parameters = 76,961,152\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [135/135 00:25, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>18.765438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>22.744600</td>\n",
              "      <td>6.876162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>22.744600</td>\n",
              "      <td>5.144644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.802500</td>\n",
              "      <td>4.681398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.802500</td>\n",
              "      <td>4.561172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 47\n",
            "  Batch size = 16\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 47\n",
            "  Batch size = 16\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 47\n",
            "  Batch size = 16\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 47\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./results/checkpoint-135\n",
            "Configuration saved in ./results/checkpoint-135/config.json\n",
            "Configuration saved in ./results/checkpoint-135/generation_config.json\n",
            "Model weights saved in ./results/checkpoint-135/model.safetensors\n",
            "tokenizer config file saved in ./results/checkpoint-135/tokenizer_config.json\n",
            "Special tokens file saved in ./results/checkpoint-135/special_tokens_map.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 47\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=135, training_loss=12.312799524377894, metrics={'train_runtime': 26.0947, 'train_samples_per_second': 80.284, 'train_steps_per_second': 5.173, 'total_flos': 97360151838720.0, 'train_loss': 12.312799524377894, 'epoch': 5.0})"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsqfLXvLh3b5",
        "outputId": "a4143cc5-b215-4ad0-bef8-aeafd8398393"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Given the following context:\n",
            "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
            "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
            "Answer: \\( 3 \\times 2+(4-5) \\)\n",
            "Please predict the misconception.\n",
            "Generated Answer: the ability to predict\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
            "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
            "Answer: \\( 3 \\times(2+4-5) \\)\n",
            "Please predict the misconception.\n",
            "Generated Answer: the ability to predict\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
            "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
            "Answer: Does not need brackets\n",
            "Please predict the misconception.\n",
            "Generated Answer: arithmetic\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
            "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
            "Answer: \\( m+1 \\)\n",
            "Please predict the misconception.\n",
            "Generated Answer: polygons\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
            "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
            "Answer: \\( m+2 \\)\n",
            "Please predict the misconception.\n",
            "Generated Answer: hypothesis\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
            "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
            "Answer: \\( m-1 \\)\n",
            "Please predict the misconception.\n",
            "Generated Answer: polygons\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
            "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
            "Answer: Only Tom\n",
            "Please predict the misconception.\n",
            "Generated Answer: are in the same height\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
            "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
            "Answer: Both Tom and Katie\n",
            "Please predict the misconception.\n",
            "Generated Answer: are in the same height\n",
            "\n",
            "Question: Given the following context:\n",
            "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
            "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
            "Answer: Neither is correct\n",
            "Please predict the misconception.\n",
            "Generated Answer: are in the same height\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert input_ids from list to torch tensor\n",
        "import torch\n",
        "\n",
        "input_ids_tensor = torch.tensor(test_dataset['input_ids']).to(device)\n",
        "\n",
        "# Generate answers using the model's generate() function\n",
        "generated_outputs = model.generate(input_ids_tensor)\n",
        "\n",
        "# Decode the generated sequences into readable text\n",
        "generated_answers = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
        "\n",
        "# Print the results\n",
        "for i, answer in enumerate(generated_answers):\n",
        "    print(f\"Question: {test_df['question_text'][i]}\")\n",
        "    print(f\"Generated Answer: {answer}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHEaD-HIh3b5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}