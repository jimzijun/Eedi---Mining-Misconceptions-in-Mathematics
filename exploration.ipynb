{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimzijun/Eedi---Mining-Misconceptions-in-Mathematics/blob/main/exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GgbGM94Zh3b3"
      },
      "outputs": [],
      "source": [
        "#!pip install kaggle scikit-learn datasets transformers torch accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ywD6A7E-h3b4"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions download -c eedi-mining-misconceptions-in-mathematics -p ./datasets/eedi\n",
        "# !unzip ./datasets/eedi/eedi-mining-misconceptions-in-mathematics.zip -d ./datasets/eedi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fitim19BiduB",
        "outputId": "7860ea76-71f7-4c4e-a98e-fc82d2d2cdb0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1JlPEyXOh3b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if a GPU is available and move model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocressing"
      ],
      "metadata": {
        "id": "--nbS-q7HOfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV files\n",
        "dataset_root_path = '/content/drive/MyDrive/datasets/eedi'\n",
        "\n",
        "df = pd.read_csv(f'{dataset_root_path}/train.csv')\n",
        "misconception_df = pd.read_csv(f'{dataset_root_path}/misconception_mapping.csv')\n",
        "sample_submission_df = pd.read_csv(f'{dataset_root_path}/sample_submission.csv')\n",
        "\n",
        "# Sample and split the train data\n",
        "train_df = df.sample(n=200, random_state=42)\n",
        "train_df, test_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "test_df, eval_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
        "\n",
        "def preprocess_dataframe(df, misconception_df=None):\n",
        "    # List of options\n",
        "    options = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    # List of columns to keep\n",
        "    id_vars = ['QuestionId', 'QuestionText', 'ConstructId', 'ConstructName', 'SubjectId', 'SubjectName', 'CorrectAnswer']\n",
        "\n",
        "    # Initialize an empty list to collect data\n",
        "    data_list = []\n",
        "\n",
        "    # Loop over each option to collect data\n",
        "    for option in options:\n",
        "        answer_col = f'Answer{option}Text'\n",
        "        misconception_col = f'Misconception{option}Id'\n",
        "\n",
        "        # Check if the misconception column exists\n",
        "        if misconception_col in df.columns:\n",
        "            temp_df = df[id_vars + [misconception_col, answer_col]].copy()\n",
        "            temp_df.rename(columns={\n",
        "                misconception_col: 'MisconceptionId',\n",
        "                answer_col: 'AnswerText'\n",
        "            }, inplace=True)\n",
        "        else:\n",
        "            # Only include the answer column if misconception column doesn't exist\n",
        "            temp_df = df[id_vars + [answer_col]].copy()\n",
        "            temp_df['MisconceptionId'] = None  # Assign None to MisconceptionId\n",
        "            temp_df.rename(columns={\n",
        "                answer_col: 'AnswerText'\n",
        "            }, inplace=True)\n",
        "\n",
        "        temp_df['Option'] = option\n",
        "        data_list.append(temp_df)\n",
        "\n",
        "    # Concatenate all the data into a single DataFrame\n",
        "    df_combined = pd.concat(data_list, ignore_index=True)\n",
        "\n",
        "    # Exclude the rows where the option matches the correct answer\n",
        "    df_combined = df_combined[df_combined['Option'] != df_combined['CorrectAnswer']]\n",
        "\n",
        "    df_combined = df_combined.merge(misconception_df, on='MisconceptionId', how='left')\n",
        "\n",
        "    # Rename 'MisconceptionName' to 'misconceptionText'\n",
        "    df_combined = df_combined.rename(columns={'MisconceptionName': 'MisconceptionText'})\n",
        "\n",
        "    # Drop rows with missing 'misconceptionText' (only for training data)\n",
        "    if 'MisconceptionText' in df_combined.columns:\n",
        "        df_combined = df_combined.dropna(subset=['MisconceptionText'])\n",
        "\n",
        "    return df_combined"
      ],
      "metadata": {
        "id": "k8kzy7BjmGmf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "0d-nHxZsHcG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from torch.nn import MSELoss\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "tUe-r6xqT9YG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagnosticQuestionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        # Store the dataframe\n",
        "        self.data = dataframe\n",
        "        # Load pre-trained BERT tokenizer and model for embedding\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model.eval()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract the row at the given index\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Extract the relevant features for the model input\n",
        "        construct = row['ConstructName']\n",
        "        subject = row['SubjectName']\n",
        "        question = row['QuestionText']\n",
        "        answer = row['AnswerText']\n",
        "        misconception = row['MisconceptionText']\n",
        "\n",
        "        # Concatenate the sequences with [CLS] and [SEP] tokens\n",
        "        input_text = f\"[CLS] {construct} [SEP] {subject} [SEP] {question} [SEP] {answer} [SEP]\"\n",
        "\n",
        "        # Tokenize the misconception text and get BERT embeddings\n",
        "        inputs = self.tokenizer(misconception, return_tensors='pt', truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.bert_model(**inputs)\n",
        "            misconception_embedding = outputs.last_hidden_state[:, 0, :].squeeze()  # Use [CLS] token representation\n",
        "\n",
        "        return {\n",
        "            'input_text': input_text,\n",
        "            'misconception_embedding': misconception_embedding,\n",
        "            'misconception_text': misconception\n",
        "        }\n",
        "\n",
        "def load_data(dataframe, batch_size=4, shuffle=True):\n",
        "    # Create the dataset\n",
        "    dataset = DiagnosticQuestionDataset(dataframe)\n",
        "\n",
        "    # Create the dataloader\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# Example usage\n",
        "train_processed_df = preprocess_dataframe(train_df, misconception_df)\n",
        "eval_processed_df = preprocess_dataframe(eval_df, misconception_df)\n",
        "test_processed_df = preprocess_dataframe(test_df, misconception_df)\n",
        "\n",
        "dataloader = load_data(train_processed_df)\n",
        "test_dataloader = load_data(test_processed_df, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZBFEg-CNhDC",
        "outputId": "86f5de8b-3100-46c9-f860-ec3eb9c2ae19"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT tokenizer and model for regression\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')  # Using BERT for regression\n",
        "\n",
        "# Add a linear layer for regression\n",
        "regression_head = torch.nn.Linear(model.config.hidden_size, model.config.hidden_size)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "regression_head.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training parameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = AdamW(list(model.parameters()) + list(regression_head.parameters()), lr=2e-5)\n",
        "epochs = 3\n",
        "criterion = MSELoss()\n",
        "\n",
        "# Store training misconception embeddings and corresponding texts for lookup\n",
        "training_misconception_dict = {}\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "regression_head.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        # Tokenize the input text\n",
        "        inputs = tokenizer(batch['input_text'], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "        # Move input tensors to the appropriate device\n",
        "        input_ids = inputs['input_ids'].to(device)\n",
        "        attention_mask = inputs['attention_mask'].to(device)\n",
        "        target_embedding = batch['misconception_embedding'].to(device)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through BERT\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predicted_embedding = regression_head(outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        # Compute loss using mean squared error\n",
        "        loss = criterion(predicted_embedding, target_embedding)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Store training misconception embeddings and texts in a dictionary (latest embedding overwrites previous one)\n",
        "        misconception_texts = batch['misconception_text']\n",
        "        for misconception_text, target_emb in zip(misconception_texts, target_embedding):\n",
        "            training_misconception_dict[misconception_text] = target_emb.cpu().numpy()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1} Loss: {avg_loss}\")\n",
        "\n",
        "# Convert training embeddings dictionary to numpy array and list of texts\n",
        "training_misconception_texts = list(training_misconception_dict.keys())\n",
        "training_misconception_embeddings = np.vstack(list(training_misconception_dict.values()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hX1PXgPNjm7",
        "outputId": "85197413-f003-42aa-e5cb-9d1f1c1405aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Training Epoch 1: 100%|██████████| 105/105 [01:08<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.125529942519608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 105/105 [01:05<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 0.06407153368705795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 105/105 [01:04<00:00,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 0.06155384204217366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_misconception_texts = list(training_misconception_dict.keys())\n",
        "training_misconception_embeddings = np.vstack(list(training_misconception_dict.values()))"
      ],
      "metadata": {
        "id": "bCIN68KVQcsO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regression_head.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYD2wifnObIf",
        "outputId": "d806ddce-6bf0-433a-8a3e-622b7b0a6077"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=768, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW83jkv7TkDW",
        "outputId": "a1c7ab8a-eb40-48a9-dd87-bf531f78024f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(test_dataloader):\n",
        "    if i >= 10:\n",
        "        break\n",
        "\n",
        "    print(f\"========== Batch {i + 1} ==========\")\n",
        "\n",
        "    input_text = batch['input_text'][0]\n",
        "    label = batch['misconception_text'][0]\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        predicted_embedding = regression_head(outputs.last_hidden_state[:, 0, :]).cpu().numpy()\n",
        "\n",
        "    # Calculate cosine similarity between the predicted embedding and all training misconception embeddings\n",
        "    similarities = cosine_similarity(predicted_embedding.reshape(1, -1), training_misconception_embeddings)\n",
        "    top_10_indices = np.argsort(similarities[0])[-10:][::-1]\n",
        "\n",
        "    # Check if label is in training misconception texts\n",
        "    if label in training_misconception_texts:\n",
        "        print(\"||||| Does exist |||||\")\n",
        "    else:\n",
        "        print(\"||||| Does not exist |||||\")\n",
        "\n",
        "    # Retrieve the top 10 closest misconception texts from the training set\n",
        "    closest_misconception_texts = [training_misconception_texts[idx] for idx in top_10_indices]\n",
        "\n",
        "    print(\"Input Text:\", input_text)\n",
        "    print(\"Label:\", label)\n",
        "    print(\"Top 10 Closest Misconception Texts:\")\n",
        "    for j, text in enumerate(closest_misconception_texts, 1):\n",
        "        print(f\"{j}: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3cZgxLZN4ho",
        "outputId": "66d0db7d-98fe-44de-9dda-a38274b04629"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== Batch 1 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Express one quantity as a fraction of another [SEP] Simplifying Fractions [SEP] Write \\( 16 \\) over \\( 28 \\) as a fraction in its simplest terms. [SEP] \\( \\frac{16}{28} \\) [SEP]\n",
            "Label: Forgot to simplify the fraction\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks a negative x value in a column vector means to move right rather than left \n",
            "2: Thinks a positive x value in a column vector means to move left rather than right\n",
            "3: Finds an equivalent improper fraction when asked for a mixed number\n",
            "4: Multiplies by the denominator instead of dividing when finding a fraction of an amount\n",
            "5: Thinks the shape of the graph does not affect the area calculation\n",
            "6: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "7: Thinks a positive y value in a column vector means to move down rather than up \n",
            "8: Does not think a square root can be negative\n",
            "9: Does not interpret the correct order of operations from a worded problem\n",
            "10: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "========== Batch 2 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Identify a kite from a description of the properties [SEP] Properties of Polygons [SEP] Which of the following quadrilaterals could not be classified as a kite? [SEP] Square [SEP]\n",
            "Label: Does not recognise the properties of a kite\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks the shape of the graph does not affect the area calculation\n",
            "2: Does not think a square root can be negative\n",
            "3: Thinks a negative x value in a column vector means to move right rather than left \n",
            "4: Does not interpret the correct order of operations from a worded problem\n",
            "5: Thinks a positive x value in a column vector means to move left rather than right\n",
            "6: Confuses the sign for parallel lines as being dashes\n",
            "7: Confuses the meaning of parallel and perpendicular\n",
            "8: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "9: Does not understand place value within a number\n",
            "10: Does not understand what each part of a bar model represents as a fraction\n",
            "========== Batch 3 ==========\n",
            "||||| Does exist |||||\n",
            "Input Text: [CLS] Carry out division  problems involving one negative integer [SEP] Adding and Subtracting Negative Numbers [SEP] \\(\n",
            "n-2 \\geq \\frac{n}{2}\n",
            "\\)\n",
            "\n",
            "If \\( n \\) can represent any number, is the statement above... [SEP] always true [SEP]\n",
            "Label: Assumes a fact without considering enough examples\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks a negative x value in a column vector means to move right rather than left \n",
            "2: Thinks a positive x value in a column vector means to move left rather than right\n",
            "3: Thinks the shape of the graph does not affect the area calculation\n",
            "4: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "5: Finds an equivalent improper fraction when asked for a mixed number\n",
            "6: Thinks a positive y value in a column vector means to move down rather than up \n",
            "7: Multiplies by the denominator instead of dividing when finding a fraction of an amount\n",
            "8: Does not think a square root can be negative\n",
            "9: Does not interpret the correct order of operations from a worded problem\n",
            "10: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "========== Batch 4 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Estimate the size of obtuse angles [SEP] Types, Naming and Estimating [SEP] Which of the following is the best estimate of the size of this angle? ![An obtuse angle]() [SEP] \\( 60^{\\circ} \\) [SEP]\n",
            "Label: Estimates angle without using knowledge of degrees in quarter turns\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks the shape of the graph does not affect the area calculation\n",
            "2: Thinks a negative x value in a column vector means to move right rather than left \n",
            "3: Thinks a positive x value in a column vector means to move left rather than right\n",
            "4: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "5: Finds an equivalent improper fraction when asked for a mixed number\n",
            "6: Thinks a positive y value in a column vector means to move down rather than up \n",
            "7: Does not think a square root can be negative\n",
            "8: Does not interpret the correct order of operations from a worded problem\n",
            "9: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "10: Believes the square of a negative will also be negative\n",
            "========== Batch 5 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Carry out a subtraction problem with positive integers where the answer is less than 0 [SEP] Adding and Subtracting Negative Numbers [SEP] Tom and Katie are trying to write calculations based on the diagram below.\n",
            "\n",
            "Tom writes: \\( 15-20=-5 \\)\n",
            "\n",
            "Katie writes: \\( (-5)+20=15 \\)\n",
            "\n",
            "Who is correct? ![Number line with an arrow marked -20 going from 15 to -5]() [SEP] Only Tom [SEP]\n",
            "Label: Does not recognise that addition is the inverse operation of subtraction\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks a negative x value in a column vector means to move right rather than left \n",
            "2: Thinks a positive x value in a column vector means to move left rather than right\n",
            "3: Finds an equivalent improper fraction when asked for a mixed number\n",
            "4: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "5: Thinks the shape of the graph does not affect the area calculation\n",
            "6: Thinks a positive y value in a column vector means to move down rather than up \n",
            "7: Does not interpret the correct order of operations from a worded problem\n",
            "8: Multiplies by the denominator instead of dividing when finding a fraction of an amount\n",
            "9: Does not think a square root can be negative\n",
            "10: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "========== Batch 6 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Factorise a quadratic expression in the form x² + bx + c [SEP] Factorising into a Double Bracket [SEP] When factorising \\( x^{2}+5 x+6 \\) we need two numbers that: [SEP] Add to give \\( 6 \\) and multiply to give \\( 5 \\) [SEP]\n",
            "Label: Does not know that to factorise a quadratic expression, to find two numbers that add to give the coefficient of the x term, and multiply to give the non variable term\n",
            "\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks a negative x value in a column vector means to move right rather than left \n",
            "2: Thinks a positive x value in a column vector means to move left rather than right\n",
            "3: Thinks a positive y value in a column vector means to move down rather than up \n",
            "4: Thinks the shape of the graph does not affect the area calculation\n",
            "5: Finds an equivalent improper fraction when asked for a mixed number\n",
            "6: Makes the number negative when asked to find the reciprocal\n",
            "7: Does not interpret the correct order of operations from a worded problem\n",
            "8: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "9: Multiplies by the denominator instead of dividing when finding a fraction of an amount\n",
            "10: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "========== Batch 7 ==========\n",
            "||||| Does exist |||||\n",
            "Input Text: [CLS] Calculate the area of a parallelogram where the dimensions are given in the same units [SEP] Area of Simple Shapes [SEP] What is the area of the parallelogram? ![A parallelogram with length 7cm, perpendicular height (marked by a right angle) 3cm, and slanted height 4cm. Not to scale]() [SEP] \\( 22 \\mathrm{~cm}^{2} \\) [SEP]\n",
            "Label: Calculates perimeter when asked for area\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks the shape of the graph does not affect the area calculation\n",
            "2: Thinks a negative x value in a column vector means to move right rather than left \n",
            "3: Thinks a positive x value in a column vector means to move left rather than right\n",
            "4: Thinks a positive y value in a column vector means to move down rather than up \n",
            "5: Does not think a square root can be negative\n",
            "6: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "7: Does not interpret the correct order of operations from a worded problem\n",
            "8: Finds an equivalent improper fraction when asked for a mixed number\n",
            "9: Confuses the meaning of parallel and perpendicular\n",
            "10: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "========== Batch 8 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Express one quantity as a fraction of another [SEP] Simplifying Fractions [SEP] Write \\( 16 \\) over \\( 28 \\) as a fraction in its simplest terms. [SEP] \\( \\frac{1}{13} \\) [SEP]\n",
            "Label: Simplifies a fraction by adding or subtracting the same amount from the numerator  and denominator\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks a negative x value in a column vector means to move right rather than left \n",
            "2: Thinks a positive x value in a column vector means to move left rather than right\n",
            "3: Finds an equivalent improper fraction when asked for a mixed number\n",
            "4: Multiplies by the denominator instead of dividing when finding a fraction of an amount\n",
            "5: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "6: Thinks the shape of the graph does not affect the area calculation\n",
            "7: Thinks a positive y value in a column vector means to move down rather than up \n",
            "8: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "9: Does not interpret the correct order of operations from a worded problem\n",
            "10: Makes the number negative when asked to find the reciprocal\n",
            "========== Batch 9 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Identify a kite from a description of the properties [SEP] Properties of Polygons [SEP] Which of the following quadrilaterals could not be classified as a kite? [SEP] Rhombus [SEP]\n",
            "Label: Does not recognise the properties of a kite\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks the shape of the graph does not affect the area calculation\n",
            "2: Does not think a square root can be negative\n",
            "3: Thinks a negative x value in a column vector means to move right rather than left \n",
            "4: Thinks a positive x value in a column vector means to move left rather than right\n",
            "5: Finds an equivalent improper fraction when asked for a mixed number\n",
            "6: Confuses the meaning of parallel and perpendicular\n",
            "7: Thinks a positive y value in a column vector means to move down rather than up \n",
            "8: Does not interpret the correct order of operations from a worded problem\n",
            "9: Confuses the sign for parallel lines as being dashes\n",
            "10: Identifies the wrong part of a fraction model as the whole\n",
            "========== Batch 10 ==========\n",
            "||||| Does not exist |||||\n",
            "Input Text: [CLS] Estimate the size of obtuse angles [SEP] Types, Naming and Estimating [SEP] Which of the following is the best estimate of the size of this angle? ![An obtuse angle]() [SEP] \\( 90^{\\circ} \\) [SEP]\n",
            "Label: Estimates angle without using knowledge of degrees in quarter turns\n",
            "Top 10 Closest Misconception Texts:\n",
            "1: Thinks the shape of the graph does not affect the area calculation\n",
            "2: Thinks a negative x value in a column vector means to move right rather than left \n",
            "3: Thinks a positive x value in a column vector means to move left rather than right\n",
            "4: Finds an equivalent improper fraction when asked for a mixed number\n",
            "5: Does not think a square root can be negative\n",
            "6: Does not interpret the correct order of operations from a worded problem\n",
            "7: Thinks a positive y value in a column vector means to move down rather than up \n",
            "8: Thinks you can divide terms by different factors when simplifying an algebraic fraction\n",
            "9: When finding the differences between terms in a sequence, believes they can do so from right to left \n",
            "10: Confuses the sign for parallel lines as being dashes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OXuApSVMTrGc"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}