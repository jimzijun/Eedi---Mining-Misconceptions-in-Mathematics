{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle scikit-learn datasets transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c eedi-mining-misconceptions-in-mathematics -p ./datasets/eedi\n",
    "# !unzip ./datasets/eedi/eedi-mining-misconceptions-in-mathematics.zip -d ./datasets/eedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available and move model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('datasets/eedi/train.csv')\n",
    "test_df = pd.read_csv('datasets/eedi/test.csv')\n",
    "misconception_df = pd.read_csv('datasets/eedi/misconception_mapping.csv')\n",
    "sample_submission_df = pd.read_csv('datasets/eedi/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = train_df.sample(n=200, random_state=42)\n",
    "# Splitting 10% for evaluation, 90% for training\n",
    "train_df, eval_df = train_test_split(train_df, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_dataframe(df, misconception_df=None, train_flag=False):\n",
    "    # List of options\n",
    "    options = ['A', 'B', 'C', 'D']\n",
    "\n",
    "    # List of columns to keep\n",
    "    id_vars = ['QuestionId', 'QuestionText', 'ConstructId', 'ConstructName', 'SubjectId', 'SubjectName', 'CorrectAnswer']\n",
    "\n",
    "    # Initialize an empty list to collect data\n",
    "    data_list = []\n",
    "\n",
    "    # Loop over each option to collect data\n",
    "    for option in options:\n",
    "        answer_col = f'Answer{option}Text'\n",
    "        misconception_col = f'Misconception{option}Id'\n",
    "        \n",
    "        # Check if the misconception column exists\n",
    "        if misconception_col in df.columns:\n",
    "            temp_df = df[id_vars + [misconception_col, answer_col]].copy()\n",
    "            temp_df.rename(columns={\n",
    "                misconception_col: 'MisconceptionId',\n",
    "                answer_col: 'AnswerText'\n",
    "            }, inplace=True)\n",
    "        else:\n",
    "            # Only include the answer column if misconception column doesn't exist\n",
    "            temp_df = df[id_vars + [answer_col]].copy()\n",
    "            temp_df['MisconceptionId'] = None  # Assign None to MisconceptionId\n",
    "            temp_df.rename(columns={\n",
    "                answer_col: 'AnswerText'\n",
    "            }, inplace=True)\n",
    "        \n",
    "        temp_df['Option'] = option\n",
    "        data_list.append(temp_df)\n",
    "\n",
    "    # Concatenate all the data into a single DataFrame\n",
    "    df_combined = pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "    # Exclude the rows where the option matches the correct answer\n",
    "    df_combined = df_combined[df_combined['Option'] != df_combined['CorrectAnswer']]\n",
    "\n",
    "    # If train_flag is True, merge with 'misconception_df' on 'MisconceptionId'\n",
    "    if train_flag and misconception_df is not None and 'MisconceptionId' in df_combined.columns:\n",
    "        df_combined = df_combined.merge(misconception_df, on='MisconceptionId', how='left')\n",
    "        \n",
    "        # Drop rows with missing 'MisconceptionName' (only for training data)\n",
    "        if 'MisconceptionName' in df_combined.columns:\n",
    "            df_combined = df_combined.dropna(subset=['MisconceptionName'])\n",
    "    else:\n",
    "        # For testing data, add a placeholder for 'MisconceptionName'\n",
    "        df_combined['MisconceptionName'] = None\n",
    "\n",
    "    # Sort and reset index if desired\n",
    "    df_combined = df_combined.sort_values([\"QuestionId\", \"Option\"]).reset_index(drop=True)\n",
    "\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_procressed_df = preprocess_dataframe(train_df, misconception_df, train_flag=True)\n",
    "eval_procressed_df = preprocess_dataframe(eval_df, misconception_df, train_flag=True)\n",
    "test_procressed_df = preprocess_dataframe(test_df, misconception_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_row_for_qa_token(row, train_flag=False):\n",
    "    # Replace newlines in all relevant fields\n",
    "    construct_name = row['ConstructName'].replace('\\n', ' ')\n",
    "    subject_name = row['SubjectName'].replace('\\n', ' ')\n",
    "    question_text = row['QuestionText'].replace('\\n', ' ')\n",
    "    answer_text = row['AnswerText'].replace('\\n', ' ')\n",
    "    \n",
    "    # Create a prompt for the question_text\n",
    "    question_prompt = (f\"Given the following context:\\n\"\n",
    "                       f\"Construct: {construct_name}, Subject: {subject_name}.\\n\"\n",
    "                       f\"Question: {question_text}\\n\"\n",
    "                       f\"Answer: {answer_text}\\n\"\n",
    "                       f\"Please predict the misconception.\")\n",
    "\n",
    "    # Set answer_text as the misconception for training\n",
    "    if train_flag and row['MisconceptionName'] is not None:\n",
    "        misconception_name = row['MisconceptionName'].replace('\\n', ' ')\n",
    "    else:\n",
    "        misconception_name = ''\n",
    "\n",
    "    # Return a DataFrame with question_text (prompt) and answer_text (misconception)\n",
    "    return pd.DataFrame({\n",
    "        'question_text': [question_prompt],\n",
    "        'answer_text': [misconception_name] if train_flag else [None]  # Use None during inference\n",
    "    })\n",
    "\n",
    "# Process the entire DataFrame for QA preparation\n",
    "def process_dataframe_for_qa_token(df, train_flag=False):\n",
    "    processed_rows = []\n",
    "    \n",
    "    # Loop through each row in the input DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        processed_row = process_row_for_qa_token(row, train_flag)\n",
    "        processed_rows.append(processed_row)\n",
    "    \n",
    "    # Concatenate the results into a final DataFrame\n",
    "    final_df = pd.concat(processed_rows, ignore_index=True)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = 'data/train.csv'\n",
    "EVAL_DF = 'data/eval.csv'\n",
    "TEST_DF = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataframe_for_qa_token(train_procressed_df, train_flag=True).to_csv(TRAIN_DF, index=False)\n",
    "process_dataframe_for_qa_token(eval_procressed_df, train_flag=True).to_csv(EVAL_DF, index=False)\n",
    "process_dataframe_for_qa_token(test_procressed_df, train_flag=False).to_csv(TEST_DF, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your data\n",
    "train_df = pd.read_csv(TRAIN_DF)  # Contains 'question_text' and 'answer_text'\n",
    "eval_df = pd.read_csv(EVAL_DF)    # Optional evaluation data\n",
    "test_df = pd.read_csv(TEST_DF).fillna('')    # Contains 'question_text' only\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "if not eval_df.empty:\n",
    "    eval_dataset = Dataset.from_pandas(eval_df)\n",
    "test_dataset = Dataset.from_pandas(test_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/spiece.model\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer_config.json\n",
      "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/419 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:00<00:00, 2349.11 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:00<00:00, 2669.00 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 1261.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"question_text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # For training data, add labels\n",
    "    if \"answer_text\" in examples:\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples[\"answer_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True, remove_columns=eval_dataset.column_names)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=test_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"google/flan-t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.45.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\" if not eval_df.empty else \"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=50,  # Log every 50 steps\n",
    "    log_level='info',  # Logging level\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 419\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 135\n",
      "  Number of trainable parameters = 76,961,152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>18.765438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>22.744600</td>\n",
       "      <td>6.876162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>22.744600</td>\n",
       "      <td>5.144644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.802500</td>\n",
       "      <td>4.681398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.802500</td>\n",
       "      <td>4.561172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-135\n",
      "Configuration saved in ./results/checkpoint-135/config.json\n",
      "Configuration saved in ./results/checkpoint-135/generation_config.json\n",
      "Model weights saved in ./results/checkpoint-135/model.safetensors\n",
      "tokenizer config file saved in ./results/checkpoint-135/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-135/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 47\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=135, training_loss=12.312799524377894, metrics={'train_runtime': 26.0947, 'train_samples_per_second': 80.284, 'train_steps_per_second': 5.173, 'total_flos': 97360151838720.0, 'train_loss': 12.312799524377894, 'epoch': 5.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/kaggle/lib/python3.12/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given the following context:\n",
      "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
      "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
      "Answer: \\( 3 \\times 2+(4-5) \\)\n",
      "Please predict the misconception.\n",
      "Generated Answer: the ability to predict\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
      "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
      "Answer: \\( 3 \\times(2+4-5) \\)\n",
      "Please predict the misconception.\n",
      "Generated Answer: the ability to predict\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Use the order of operations to carry out calculations involving powers, Subject: BIDMAS.\n",
      "Question: \\[ 3 \\times 2+4-5 \\] Where do the brackets need to go to make the answer equal \\( 13 \\) ?\n",
      "Answer: Does not need brackets\n",
      "Please predict the misconception.\n",
      "Generated Answer: arithmetic\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
      "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
      "Answer: \\( m+1 \\)\n",
      "Please predict the misconception.\n",
      "Generated Answer: polygons\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
      "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
      "Answer: \\( m+2 \\)\n",
      "Please predict the misconception.\n",
      "Generated Answer: hypothesis\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Simplify an algebraic fraction by factorising the numerator, Subject: Simplifying Algebraic Fractions.\n",
      "Question: Simplify the following, if possible: \\( \\frac{m^{2}+2 m-3}{m-3} \\)\n",
      "Answer: \\( m-1 \\)\n",
      "Please predict the misconception.\n",
      "Generated Answer: polygons\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
      "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
      "Answer: Only Tom\n",
      "Please predict the misconception.\n",
      "Generated Answer: are in the same height\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
      "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
      "Answer: Both Tom and Katie\n",
      "Please predict the misconception.\n",
      "Generated Answer: are in the same height\n",
      "\n",
      "Question: Given the following context:\n",
      "Construct: Calculate the range from a list of data, Subject: Range and Interquartile Range from a List of Data.\n",
      "Question: Tom and Katie are discussing the \\( 5 \\) plants with these heights: \\( 24 \\mathrm{~cm}, 17 \\mathrm{~cm}, 42 \\mathrm{~cm}, 26 \\mathrm{~cm}, 13 \\mathrm{~cm} \\) Tom says if all the plants were cut in half, the range wouldn't change. Katie says if all the plants grew by \\( 3 \\mathrm{~cm} \\) each, the range wouldn't change. Who do you agree with?\n",
      "Answer: Neither is correct\n",
      "Please predict the misconception.\n",
      "Generated Answer: are in the same height\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert input_ids from list to torch tensor\n",
    "import torch\n",
    "\n",
    "input_ids_tensor = torch.tensor(test_dataset['input_ids']).to(device)\n",
    "\n",
    "# Generate answers using the model's generate() function\n",
    "generated_outputs = model.generate(input_ids_tensor)\n",
    "\n",
    "# Decode the generated sequences into readable text\n",
    "generated_answers = tokenizer.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "\n",
    "# Print the results\n",
    "for i, answer in enumerate(generated_answers):\n",
    "    print(f\"Question: {test_df['question_text'][i]}\")\n",
    "    print(f\"Generated Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
